<!DOCTYPE html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="wYvWtjOgtH2vemeJWG0qdpVSuGJB5wU3lk6l4sZgnpc" />
  <meta charset="utf-8">
  <meta http-equiv="Cache-control" content="public">
  <meta name="description" content="Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients">
  <meta name="keywords" content="Property Gradients, Interventions, Causality-based Explanations, XAI, Explainability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients</title>

  <link rel="icon" href="static/images/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/jquery-3.5.1.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
    <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2024 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://inf-cv.uni-jena.de/home/staff/">
          <span class="cvg-logo"></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://causalrivers.github.io/">
              ICLR 2025 - CausalRivers: Scaling up Benchmarking of Causal Discovery
            </a>
            <a class="navbar-item" href="https://eifer-mam.github.io/">
              CVPR 2025 - EIFER: Electromyography-Informed Facial Expression Reconstruction
            </a>
            <a class="navbar-item" href="https://fastcav.github.io/">
              ICML 2025 - FastCAV: Efficient Computation of Concept Activation Vectors 
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><b>Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients</b></h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?user=s9bfMqEAAAAJ&hl=en">Niklas Penzel<sup><small>1</small></sup></a>,
              </span>
              <span class="author-block">
                <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.de/citations?hl=en&user=bhpi3vgAAAAJ&hl=en">Joachim Denzler<sup><small>1</small></sup></a>,
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup><small>1</small></sup> Computer Vision Group Jena, Friedrich Schiller University Jena, Germany
              </span>
            </div>
            <br>
            <div class="is-size-5 publication-authors is-dark is">
              <b>
                WACV 2026
              </b>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="todo" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-globe"></i>
                    </span>
                    <span>Conference</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Experiment Code Link. -->
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PropGrad/Interventional-XAI" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <h2 class="title is-2">Abstract</h2>
            <p style="font-size: 0.9rem;" class="is-centered">
              <div>&ensp;</div>
              <div>&ensp;</div>
              <div class="content has-text-justified">
              Deep learning models achieve high predictive performance but lack intrinsic interpretability, hindering our understanding of the learned prediction behavior.
              Existing local explainability methods focus on associations, neglecting the causal drivers of model predictions. 
              Other approaches adopt a causal perspective but primarily provide global, model-level explanations.
              However, for specific inputs, <strong>it's unclear whether globally identified factors apply locally.</strong>
              To address this limitation, we introduce a novel framework for local interventional explanations by leveraging recent advances in image-to-image editing models. 
              Our approach performs gradual interventions on semantic properties to <strong>quantify the corresponding impact on a model's predictions</strong> using a novel score, the <strong>expected property gradient magnitude</strong>. 
              We demonstrate the effectiveness of our approach through an extensive empirical evaluation on a wide range of architectures and tasks.
              First, we validate it in a synthetic scenario and demonstrate its <strong>ability to locally identify biases</strong>.
              Afterward, we apply our approach to investigate medical skin lesion classifiers, analyze network training dynamics, and study a pre-trained CLIP model with real-life interventional data.
              Our results highlight the potential of interventional explanations on the property level to reveal new insights into the behavior of deep models.
              </div>
            </p>
          </div>
        </div>
        <div class="column has-text-centered">
          <h2 class="title is-4">Example Interventions for <br> Three Cat vs. Dog Classifiers</h2>
          <div class="columns is-centered">
            <div class="column content">
              <!-- <div>&ensp;</div> -->
              <!-- <div>&ensp;</div> -->
              <img src="./static/images/teaser.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <p><strong>Important:</strong> We first include some results and highlights. You can find a overview for the theoretical foundation further down or simply open <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424">our paper</a> &#128519;</p>
            <hr>
            <h2 class="title is-2 fullwidth-highlight">Selected Results and Visualizations</h2>
        </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <hr>
            <h2 class="title is-2 fullwidth-highlight">Theoretical Background and Preliminaries</h2>
        </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Structural Causal Model for Property Dependence</h2>
            <img src="./static/images/scm.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>

          <div class="column">
          <div class="content">
            <p class="content has-text-justified">
              We consider a structural causal model (SCM) that describes the causal relationships between inputs, specifically, captured properties of interest X, and a model's predictions Ŷ (see Figure).
              The properties X are high-level, human-understandable features contained in images, such as object shape, color, or texture.
              The SCM captures the causal dependencies between these variables, allowing us to reason about how changes in one variable affect others.
              Dashed connections potentially exist depending on the specific task/property combination, and the sampled training data.
              In particular, we are interested in understanding how interventions on the properties X influence a model's predictions Ŷ (<strong style="color:red">red dashed link</strong>).
              Given that Ŷ is fully determined by the models, we can gain insights into the model behavior by performing targeted interventions on selected properties of interest X.
            </p>
          </div>
          </div>
        </div>
    </div>
  </section>

  
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Why Do We Need Interventions?</h2>
            <p class="content has-text-justified">
              Causal insights can be hierarchically ordered in the so-called causal ladder.
              This ladder, formally <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">Pearl's Causal Hierarchy (PCH)</a>, contains three distinct levels: associational, interventional, and counterfactual (see <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">here</a> for a formal definition).
            </p>
              <ul>
              <li>The first level, associational, is characterized by correlations observed in a given system.<br> 
              It focuses on statistical patterns and relationships within the data.</li>
              <li>The second level, interventional, involves actively changing variables within the system to study the resulting effects. 
              This is formally represented using the <a target="_blank" rel="noopener noreferrer" href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B"><italic>do</italic>-operator</a>, which allows researchers to examine the causal impact of interventions.</li>
              <li>The third level, counterfactual, deals with hypothetical scenarios, where researchers consider the potential outcome if an intervention had been made, given specific observations.</li>
              </ul>
            <p class="content has-text-justified">
              Crucially, the <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">causal hierarchy theorem</a> states that the three levels are distinct, and the PCH almost never collapses in the general case.
              Hence, to answer questions of a certain PCH level, data from the corresponding level is needed (Corollary 1 in <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">here</a>).
            </p>
          </div>
        </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Generating Interventional Data</h2>
            <p class="content has-text-justified">
              To study neural network prediction behavior on the interventional level, we have to generate interventional data with respect to selected properties.
              To achieve this, we propose three different strategies:
            </p>
              <ol>
              <li>If possible and/or feasible, we recommend capturing new interventional data. This enables users to fully control all factors and limits confounding artifacts.</li>
              <li>In cases where capturing new data is not feasible, we suggest using existing datasets and applying data augmentation techniques to create variations that reflect the desired interventions. 
                This approach enables domain experts to target specific properties of interest while saving the costs of re-collection.</li>
              <li>Finally, we can leverage recent image-to-image editing models, e.g., <a target="_blank" rel="noopener noreferrer" href="https://www.timothybrooks.com/instruct-pix2pix/">InstructPix2Pix</a>, <a target="_blank" rel="noopener noreferrer" href="https://mllm-ie.github.io/">MGIE</a>, or <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/spaces/leditsplusplus/project">LEDITS++</a>,  to synthetically generate interventional data by modifying specific properties of interest in the images.
                Further, we can perform gradual interventions by continuously varying the strength of the applied edits, e.g., by using <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.12598">classifier free guidance scaling</a>.
                These gradual interventions allow us to measure the sensitivity of a model's predictions to changes in specific properties, providing insights into the local prediction behavior.
              </li>
              </ol>

              <h2 class="title is-3">Measuring a Property's Local Impact</h2>
          </div>
        </div>
  </section>

  
  

  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <h2 class="title">BibTeX</h2>
      <p class="content has-text-justified">
        If you find our work useful, please consider citing our paper!
      </p>
      <pre>
      <code>
@inproceedings{penzel2025towards,
    author = {Niklas Penzel and Joachim Denzler},
    title = {Towards Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients},
    year = {2025},
    doi = {10.48550/arXiv.2503.05424},
    arxiv = {https://arxiv.org/abs/2503.05424},
    note = {accepted at WACV 2026},
}</code>
      </pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://icml.cc/virtual/2025/poster/44251">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/cvjena" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              The source code of this site is borrowed from <a target="_blank" rel="noopener noreferrer" href="https://github.com/nerfies/nerfies.github.io">here</a>.
              We thank its creators for their work. 
            </p>
            <br><a rel="license" href="impressum_privacy.html">Impressum and Data privacy</a>.
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
