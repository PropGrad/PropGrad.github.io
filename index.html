<!DOCTYPE html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="wYvWtjOgtH2vemeJWG0qdpVSuGJB5wU3lk6l4sZgnpc" />
  <meta charset="utf-8">
  <meta http-equiv="Cache-control" content="public">
  <meta name="description" content="Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients">
  <meta name="keywords" content="Property Gradients, Interventions, Causality-based Explanations, XAI, Explainability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Measuring Property Gradients</title>

  <link rel="icon" href="static/images/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/jquery-3.5.1.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
    <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2024 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://inf-cv.uni-jena.de/home/staff/">
          <span class="cvg-logo"></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://causalrivers.github.io/">
              ICLR 2025 - CausalRivers: Scaling up Benchmarking of Causal Discovery
            </a>
            <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://eifer-mam.github.io/">
              CVPR 2025 - EIFER: Electromyography-Informed Facial Expression Reconstruction
            </a>
            <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://fastcav.github.io/">
              ICML 2025 - FastCAV: Efficient Computation of Concept Activation Vectors
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><b>Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients</b></h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?user=s9bfMqEAAAAJ&hl=en">Niklas Penzel<sup><small>1</small></sup></a>,
              </span>
              <span class="author-block">
                <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.de/citations?hl=en&user=bhpi3vgAAAAJ&hl=en">Joachim Denzler<sup><small>1</small></sup></a>,
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup><small>1</small></sup> Computer Vision Group Jena, Friedrich Schiller University Jena, Germany
              </span>
            </div>
            <br>
            <div class="is-size-5 publication-authors is-dark is">
              <b>
                WACV 2026
              </b>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="todo" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-globe"></i>
                    </span>
                    <span>Conference</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Experiment Code Link. -->
                <span class="link-block">
                  <a target="_blank" rel="noopener noreferrer" href="https://github.com/PropGrad/Interventional-XAI" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <h2 class="title is-2">Abstract</h2>
            <p style="font-size: 0.9rem;" class="is-centered">
              <div>&ensp;</div>
              <div>&ensp;</div>
              <div class="content has-text-justified">
              Deep learning models achieve high predictive performance but lack intrinsic interpretability, hindering our understanding of the learned prediction behavior.
              Existing local explainability methods focus on associations, neglecting the causal drivers of model predictions. 
              Other approaches adopt a causal perspective but primarily provide global, model-level explanations.
              However, for specific inputs, <strong>it's unclear whether globally identified factors apply locally.</strong>
              To address this limitation, we introduce a novel framework for local interventional explanations by leveraging recent advances in image-to-image editing models. 
              Our approach performs gradual interventions on semantic properties to <strong>quantify the corresponding impact on a model's predictions</strong> using a novel score, the <strong>expected property gradient magnitude</strong>. 
              We demonstrate the effectiveness of our approach through an extensive empirical evaluation on a wide range of architectures and tasks.
              First, we validate it in a synthetic scenario and demonstrate its <strong>ability to locally identify biases</strong>.
              Afterward, we apply our approach to investigate medical skin lesion classifiers, analyze network training dynamics, and study a pre-trained CLIP model with real-life interventional data.
              Our results highlight the potential of interventional explanations on the property level to reveal new insights into the behavior of deep models.
              </div>
            </p>
          </div>
        </div>
        <div class="column has-text-centered">
          <h2 class="title is-4">Example Interventions for <br> Three Cat vs. Dog Classifiers</h2>
          <div class="columns is-centered">
            <div class="column content">
              <!-- <div>&ensp;</div> -->
              <!-- <div>&ensp;</div> -->
              <img src="./static/images/teaser.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <p><strong>Important:</strong> We first include some results and highlights. You can find a overview for the theoretical foundation further down or simply open <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424">our paper</a> &#128519;</p>
            <hr>
            <h2 class="title is-2 fullwidth-highlight">Selected Results and Visualizations</h2>
            <h2 class="title is-3">Validation in a Cats versus Dogs Classification Task With Known Bias</h2>
            <p class="content has-text-justified">
              We validate our approach in a synthetic scenario with known biases.
              Specifically, we train models on modified version of the <a target="_blank" rel="noopener noreferrer" href="https://www.kaggle.com/competitions/dogs-vs-cats">Cats vs. Dogs dataset</a>, where we introduce a spurious correlation between the class label and the animals fur color.
              In particular, we obtain three training and test data splits: the original unbiased split, a split with only dark-furred dogs and light-furred cats, and the reverse. 
              Our results demonstrate that our method can effectively identify the biased property (fur color) as significant for all three models, while correctly recognizing other properties (e.g., background brightness) as unimportant (see teaser figure above).
              Further, <strong>we can quantify the impact of the property on model predictions</strong> (below left), confirming the ability of our approach to detect locally present biases in model predictions.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Our Approach to Quantify Impact</h2>
            <img src="./static/images/cvd-propgrad-table.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        <div class="column">
          <div class="content">
            <h2 class="title is-4"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1602.04938">LIME</a> as a Local Attribution Baseline</h2>
            <img src="./static/images/lime-examples.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        </div>

      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <p class="content has-text-justified">
              Crucially, while other local attribution methods highlight important areas, they require semantic interpretation.
              For example, although the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1602.04938">LIME</a> explanations above align with fur color for biased models, the distinction from the unbiased model is unclear.
              <strong>A focus on interventions</strong>, i.e., the disparity between the top and bottom rows above left, can help interpret the results.
              You can find more comparisons and additional baselines in <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424">our paper</a>.
            </p>
          </div>
        </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Manual Interventions to Study Skin Lesion Classifiers</h2>
            <p class="content has-text-justified">
              In the domain of skin lesion classification, a known bias is the correlation between <a target="_blank" rel="noopener noreferrer" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5030195/">colorful patches</a> and the healthy nevus class.
              We assess how strongly this property is learned by four different ImageNet pretrained architectures.
              Then, we either fine-tune on biased skin lesion data (50% nevi with <a target="_blank" rel="noopener noreferrer" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5030195/">colorful patches</a>) or unbiased data (no patches) from the <a target="_blank" rel="noopener noreferrer" href="https://www.isic-archive.com/">ISIC archive</a>.
              To demonstrate that <strong>our approach accommodates diverse sources of interventional data</strong>, we build on domain knowledge and intervene synthetically.
              Specifically, we blend <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13584">segmented colorful patches</a> into melanoma images.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Visualized Average Behavior for ResNet18</h2>
            <img src="./static/images/isic-explanation.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Estimated Impact for Different Melanoma Classifiers Trained on Different Data Sources</h2>
            <div>&ensp;</div>
            <img src="./static/images/isic-table.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />
            </div>
          </div>
        </div>



      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <p class="content has-text-justified">
              The mean expected property gradient magnitudes in the table above show that models trained on biased data are most impacted by colorful patch interventions, indicating they learn the statistical correlation between the patches and the nevus class. 
              Furthermore, the variants with ImageNet weights show higher patch sensitivity than the unbiased skin lesion models. 
              We hypothesize this is because learning color is beneficial for general-purpose pre-training, whereas the unbiased models learn to disregard patches and focus on the actual lesions.
            </p>

            <h2 class="title is-3">Quantitative Comparisons</h2>

            <p class="content has-text-justified">
              To  quantitatively compare our approach to local baselines, <strong>we propose a downstream task inspired by <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.07421">insertion/deletion tests</a>: predicting whether an intervention on a property will change the model's output.</strong>
              This task directly assesses if a method can indicate locally biased behavior and allows for a quantitative comparison to local baselines, given that our approach does not produce saliency maps but rather estimates the impact of a property directly using property gradients.
              To adapt saliency methods, we measure the mean squared difference of the explanation pre- and post-intervention.
              For a fair comparison, we select the optimal threshold maximizing the accuracy for both the local baselines and our score.
            </p>
            <img src="./static/images/downstream-table.png" class="interpolation-image" alt="Downstream Task Performance" width="1400" height="1400" />
            <p class="content has-text-justified">
              Our results show that <strong>our approach outperforms all local baselines</strong> in both the synthetically biased cats versus dogs dataset and the realistic skin lesion task.
              However, our aim is not to replace saliency methods, but rather to offer <strong>a complementary, interventional viewpoint for analyzing local behavior.</strong>
              We demonstrate these capabilities with our analysis of neural network training and <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.00020">CLIP</a> models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Analyzing Neural Network Training Dynamics</h2>
            <p class="content has-text-justified">
            In this section, we analyze the training dynamics of neural networks using our proposed interventional framework.
            Specifically, we investigate <strong>how the sensitivity of a model to a selected property evolves during training.</strong>
            We select a range of convolutional and transformer-based architectures widely used in computer vision tasks. 
            For all of these models, we train a randomly initialized and an ImageNet pre-trained version for 100 epochs.
            </p>
            <p class="content has-text-justified">
            Regarding the corresponding task, we construct a binary classification problem from <a target="_blank" rel="noopener noreferrer" href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>, following an idea proposed in <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2403.09683">here</a>.
            Specifically, we utilize the attribute young as a label and split the data in a balanced manner.
            For this label, the gray hair property is negatively correlated, and a well-performing classifier should learn this association during the training process.
            To study the training dynamics, <strong>we intervene on the hair color and calculate expected property gradient magnitudes after each epoch.</strong>
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <img src="./static/images/training-ana-corr.png" class="interpolation-image" alt="Teaser Figure" width="1200" height="1000" />
          </div>
        </div>
        <div class="column">
          <div class="content">
            <div>&ensp;</div>
            <div>&ensp;</div>
            <img src="./static/images/training-ana-densenet.png" class="interpolation-image" alt="Teaser Figure" width="1200" height="1000" />
          </div>
        </div>

      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <p class="content has-text-justified">
            We visualize the average expected property gradient magnitudes of the hair color for a local example over the training process for both pre-trained and randomly initialized models.
            Specifically, we display the average against the observed flips in the prediction during the hair color intervention (above left).
            <strong>Our analysis reveals two key insights:</strong>
            </p>
            <ul>
              <li>
                First, for all architectures, the pre-trained variants locally exhibit higher expected property gradient magnitudes compared to the randomly initialized versions. 
                This observation is consistent with the number of times the networks' predictions flip during training. 
                In general, we find that <strong>increased measured impact correlates with more flipped predictions (above left).</strong>
              </li>
              <li>
                Our second key finding is illustrated in the visualization above right, which reveals that the impact of the hair color exhibits strong local fluctuations during the training for the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.06993">DenseNets</a>.
                In other words, the networks <strong>do not continuously learn to rely on the hair color property but instead locally "forget" it, even in later epochs.</strong>
                This effect is particularly pronounced for the pre-trained model, whereas the randomly initialized version tends to show lower expected property gradient magnitudes.
                We observe similar behavior for other architectures.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Analyzing a <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.00020">CLIP</a> Model with Real World Interventions</h2>
            <p class="content has-text-justified">
            In our final set of experiments, we investigate the widely used multimodal backbone <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.00020">CLIP</a> <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.11929">ViT-B/32</a> for zero-shot classification.
            <strong>Our approach is model-agnostic</strong>, requiring only access to model outputs, here cosine similarities in the learned latent space.
            As the property of interest, we select object orientation, which is a known bias, for example, <a target="_blank" rel="noopener noreferrer" href="https://jov.arvojournals.org/article.aspx?articleid=2776554">in ImageNet models</a>.
            Additionally, we demonstrate a third type of interventional data and capture real-life interventional images.
            Specifically, <strong>we record a rotation of three toy figures (elephant, giraffe, and stegosaurus) using a turntable.</strong>
            </p>
            <img src="./static/images/stego.png" class="interpolation-image" alt="Stegosaurus Rotation" width="1200" height="1200" />
            <p class="content has-text-justified">
            Note that in our experiments the behavior is remarkably consistent between text descriptors with similar standard deviations during the full interventions.
            Further, <strong>all measured expected property gradient magnitudes are statistically significant (p &lt; 0.01)</strong>, i.e., the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.00020">CLIP</a> model is influenced by object orientation.
            While expected, our local interventional approach facilitates direct interpretations of the change in behavior.
            </p>
            <p class="content has-text-justified">
            The figure below shows the highest average similarity for the correct class occurs when the toy animal is rotated sideways.
            Periodical minima align with the front or back-facing orientations. 
            In contrast, the highest similarities for the other classes appear close to the minimum of the ground truth, indicating lower confidence.
            </p>
            <img src="./static/images/clip-stego.png" class="interpolation-image" alt="Teaser Figure" width="1000" height="1000" />

            <p class="content has-text-justified">
            To further validate these results, we include the response to synthetic rotations of a 3D model around other axes as additional ablations (see below).
            We confirm that uncommon, e.g., upside-down positions, lead to lower scores (we mark minima).
            Hence, <strong>our approach provides actionable guidance to locally select an appropriate input orientation.</strong>
            </p>
            <img src="./static/images/clip-frog.png" class="interpolation-image" alt="Teaser Figure" width="1020" height="1000" />
          </div>
        </div>
      </div>
    </div>
  </section>






  <!-- Theoretical Background & Preliminaries -->


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column has-text-centered">
          <div class="content">
            <hr>
            <h2 class="title is-2 fullwidth-highlight">Theoretical Background and Preliminaries</h2>
            <h2 class="title is-3">Why Do We Need Interventions?</h2>
            <p class="content has-text-justified">
              Causal insights can be hierarchically ordered in the so-called causal ladder.
              This ladder, formally <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">Pearl's Causal Hierarchy (PCH)</a>, contains three distinct levels: <strong>associational, interventional, and counterfactual</strong> (see <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">here</a> for a formal definition).
            </p>
              <ul>
              <li>The first level, associational, is characterized by correlations observed in a given system.<br> 
              It focuses on statistical patterns and relationships within the data.</li>
              <li>The second level, interventional, involves actively changing variables within the system to study the resulting effects. 
              This is formally represented using the <a target="_blank" rel="noopener noreferrer" href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B"><italic>do</italic>-operator</a>, which allows researchers to examine the causal impact of interventions.</li>
              <li>The third level, counterfactual, deals with hypothetical scenarios, where researchers consider the potential outcome if an intervention had been made, given specific observations.</li>
              </ul>
            <p class="content has-text-justified">
              Crucially, the <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">causal hierarchy theorem</a> states that <strong>the three levels are distinct, and the PCH almost never collapses in the general case.</strong>
              Hence, to answer questions of a certain PCH level, data from the corresponding level is needed (Corollary 1 in <a target="_blank" rel="noopener noreferrer" href="https://causalai.net/r60.pdf">here</a>).
            </p>

            <div>&ensp;</div>
            <div>&ensp;</div>
          </div>
        </div>

      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Structural Causal Model for Property Dependence</h2>
            <img src="./static/images/scm.png" class="interpolation-image" alt="Structural Causal Model" width="1000" height="1000" />
            </div>
          </div>

          <div class="column">
          <div class="content">
            <p class="content has-text-justified">
              We consider a structural causal model (SCM) that describes the causal relationships between inputs, specifically, captured properties of interest X, and a model's predictions Ŷ (see Figure).
              The properties X are high-level, human-understandable features contained in images, such as object shape, color, or texture.
              The SCM captures the causal dependencies between these variables, allowing us to reason about how changes in one variable affect others.
              Dashed connections potentially exist depending on the specific task/property combination, and the sampled training data.
              In particular, we are interested in understanding how interventions on the properties X influence a model's predictions Ŷ (<strong style="color:red">red dashed link</strong>).
              Given that Ŷ is fully determined by the models, we can gain insights into the model behavior by performing targeted interventions on selected properties of interest X.
            </p>
          </div>
          </div>
        </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Generating Interventional Data</h2>
            <p class="content has-text-justified">
              To study neural network prediction behavior on the interventional level, we have to generate interventional data with respect to selected properties.
              To achieve this, we propose three different strategies:
            </p>
            <ol>
              <li>If possible and/or feasible, we recommend capturing new interventional data. This enables users to fully control all factors and limits confounding artifacts.</li>
              <li>In cases where capturing new data is not feasible, we suggest using existing datasets and applying data augmentation techniques to create variations that reflect the desired interventions. 
                This approach enables domain experts to target specific properties of interest while saving the costs of re-collection.</li>
              <li>Finally, we can leverage recent image-to-image editing models, e.g., <a target="_blank" rel="noopener noreferrer" href="https://www.timothybrooks.com/instruct-pix2pix/">InstructPix2Pix</a>, <a target="_blank" rel="noopener noreferrer" href="https://mllm-ie.github.io/">MGIE</a>, or <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/spaces/leditsplusplus/project">LEDITS++</a>,  to synthetically generate interventional data by modifying specific properties of interest in the images.
                Further, we can perform gradual interventions by continuously varying the strength of the applied edits, e.g., by using <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.12598">classifier free guidance scaling</a>.
                These gradual interventions allow us to measure the sensitivity of a model's predictions to changes in specific properties, providing insights into the local prediction behavior.
              </li>
            </ol>

            <h2 class="title is-3">Measuring a Property's Local Impact</h2>
            <p class="content has-text-justified">
              To measure the changes induced in the network outputs for interventions in property X of a given input image, we approximate the magnitude of the corresponding gradient. 
              Gradients as a measure of change or impact with respect to X are related to the <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.07165">causal concept effect</a> and can be seen as an extension for gradual interventions.
              Specifically, we define the <strong>expected property gradient magnitude</strong> as follows:
            </p>
            <img src="./static/images/expropgrad.png" class="interpolation-image" alt="Expected Property Gradient Magnitude" width="450" height="450" />
            <p class="content has-text-justified">
              where we refer the reader to <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.05424">our paper</a> for further details.
              In practice, we approximate the expected value by sampling a set of N gradual interventions on property X for a given input image and compute the average absolute gradient magnitude using <a target="_blank" rel="noopener noreferrer" href="https://www.colorado.edu/amath/sites/default/files/attached-files/mathcomp_88_fd_formulas.pdf">finite differences</a>.
            </p>
          </div>
        </div>
  </section>

  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <p class="content has-text-justified">
              <strong>Crucially, a high effect size does not imply significance.</strong> 
              Hence, to determine significance, we perform shuffle hypothesis testing.
              This approach compares a test statistic from the original observations to K randomly shuffled versions.
              Here, the interventional values of the property X and the corresponding model outputs for the intervened inputs constitute the original correspondence.
              We use our expected property gradient magnitude as test statistic, which connects our measure of behavior changes to the hypothesis test.
              Permuting the observations destroys the systematic relationship between the property X and the model outputs and facilitates approximating the null hypothesis.
              In our experiments, we use a significance level of 0.01 and perform 10K permutations.
              See the following example visualizations for random noise and a sinusoidal signal:
            </p>
            <img src="./static/images/shuffle-ex.png" class="interpolation-image" alt="Shuffle Hypothesis Test Examples" width="600" height="600" />
          </div>
        </div>
        <div class="column">
          <div class="content">
            <img src="./static/images/hyptest.png" class="interpolation-image" alt="Pseudo Code" width="540" height="540" />
          </div>
        </div>
  </section>

  
  

  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <h2 class="title">BibTeX</h2>
      <p class="content has-text-justified">
        If you find our work useful, please consider citing our paper!
      </p>
      <pre>
      <code>
@inproceedings{penzel2025towards,
    author = {Niklas Penzel and Joachim Denzler},
    title = {Towards Locally Explaining Prediction Behavior via Gradual Interventions and Measuring Property Gradients},
    year = {2025},
    doi = {10.48550/arXiv.2503.05424},
    arxiv = {https://arxiv.org/abs/2503.05424},
    note = {accepted at WACV 2026},
}</code>
      </pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://icml.cc/virtual/2025/poster/44251">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/cvjena" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              The source code of this site is borrowed from <a target="_blank" rel="noopener noreferrer" href="https://github.com/nerfies/nerfies.github.io">here</a>.
              We thank its creators for their work. 
            </p>
            <br><a rel="license" href="impressum_privacy.html">Impressum and Data privacy</a>.
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
